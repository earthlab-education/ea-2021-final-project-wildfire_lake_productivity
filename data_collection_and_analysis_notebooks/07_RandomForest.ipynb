{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8ceeb8",
   "metadata": {},
   "source": [
    "# 1.  Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2dd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import io\n",
    "\n",
    "import earthpy as et\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "import graphviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be09228",
   "metadata": {},
   "source": [
    "# 2. Make/set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c244d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path exists\n"
     ]
    }
   ],
   "source": [
    "# Set working directory\n",
    "\n",
    "# if the desired path exists:\n",
    "data_dir = os.path.join(et.io.HOME, 'Dropbox',\n",
    "                        'cu_earthdata_certificate_2021', 'earthlab_project', 'data')\n",
    "if os.path.exists(data_dir):\n",
    "    # set working directory:\n",
    "    os.chdir(data_dir)\n",
    "    print(\"path exists\")\n",
    "else:\n",
    "    print(\"path does not exist, making new path\")\n",
    "    os.makedirs(data_dir)\n",
    "    os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace478b7",
   "metadata": {},
   "source": [
    "# 3. Download and organize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a8617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurenkremer/miniconda3/envs/wildfire-lake-productivity/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3170: DtypeWarning: Columns (18,28,29) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# 1.  Download landcover data generated in script 06\n",
    "file_path = os.path.join(\"lake_landcover.csv\")\n",
    "landcover_stats = pd.read_csv(file_path)\n",
    "\n",
    "landcover_stats = landcover_stats.drop(\n",
    "    ['source', 'TZID', 'clouds', 'type', 'path', 'system_ind', 'date_utc', 'timediff', 'date_unity', 'pixelCount', 'id', 'sat', 'qa_sd', 'endtime', 'row', 'swir1_sd', 'swir2_sd', 'date_only', 'blue', 'blue_sd', 'red', 'red_sd', 'green', 'green_sd'], axis=1)\n",
    "\n",
    "\n",
    "# 2.  Download color stats collected in script 03\n",
    "file_path = os.path.join(\"lake_mtbs_merged_tab.csv\")\n",
    "color_stats = pd.read_csv(file_path)\n",
    "\n",
    "color_stats = color_stats.drop(['Aerosol', 'pCount_dswe1', 'pCount_dswe3', 'sat', 'Event_ID', 'irwinID', 'Incid_Name',\n",
    "                               'Map_ID', 'Map_Prog', 'Asmnt_Type', 'Ig_Date', 'Pre_ID', 'Post_ID', 'Perim_ID', 'dNBR_stdDv'], axis=1)\n",
    "\n",
    "\n",
    "# Merge the first two datasets\n",
    "new_df = pd.merge(landcover_stats, color_stats)  #how='left', left_on=[\n",
    "    #'landsat_id', 'Hylak_id', 'date'], right_on=['LandsatID', 'Hylak_id', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d85e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Download topography data generated in script 05\n",
    "file_path = os.path.join(\"lake_topography.csv\")\n",
    "topography_stats = pd.read_csv(file_path)\n",
    "\n",
    "topography_stats = topography_stats.drop(['Unnamed: 0', 'source', 'TZID', 'clouds', 'type', 'path', 'system_ind', 'date_utc', 'timediff',\n",
    "                                         'date_unity', 'pixelCount', 'id', 'sat', 'qa_sd', 'endtime', 'row', 'swir1_sd', 'swir2_sd', 'date_only', 'geometry', 'blue', 'blue_sd', 'red', 'red_sd', 'green', 'green_sd'], axis=1)\n",
    "\n",
    "more_df = pd.merge(new_df, topography_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb659821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download area data generated in script 01\n",
    "file_path = os.path.join(\"polygon_areas_tab.csv\")\n",
    "area_stats = pd.read_csv(file_path)\n",
    "\n",
    "area_stats = area_stats.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "bigger_df = pd.merge(more_df, area_stats) #547 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "900ce976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurenkremer/miniconda3/envs/wildfire-lake-productivity/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3170: DtypeWarning: Columns (19,29,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# 5. Download time since fire data from script 02\n",
    "file_path = os.path.join(\"burned_lakes_tabular.csv\")\n",
    "time_fire_stats = pd.read_csv(file_path)\n",
    "\n",
    "small_time_stats = time_fire_stats[['Hylak_id', 'LandsatID', 'date', 'pre_post', 'days_since', 'img_month', 'years_since', \"fui\"]]\n",
    "\n",
    "full_df = pd.merge(bigger_df, small_time_stats,  how='left', left_on=[\n",
    "    'Hylak_id', 'date'], right_on=['Hylak_id', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a38177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted columns\n",
    "full_df = full_df.drop(['Hylak_id', 'landsat_id', 'date', 'LandsatID_y', 'Unnamed: 0', 'system:index', 'SiteID',\n",
    "                       'LandsatID_x', 'NoData_T', 'Mod_T', 'High_T', 'Low_T', 'IncGreen_T', 'time', 'tis'], axis=1)\n",
    "\n",
    "# Remove columns with insufficuent in-situ data\n",
    "full_df = full_df.drop(['doc', 'secchi', 'tss', 'p_sand'], axis=1)\n",
    "\n",
    "# Remove columns that may be correlated with color \n",
    "full_df = full_df.drop(['nir_sd', 'Blue', 'Red', 'Green', 'fui', 'Swir1', 'Swir2', 'Nir', 'swir2', 'nir', 'TIR2', 'TIR1', 'swir1'], axis=1)\n",
    "\n",
    "# Lets drop columns where chla data is empty.  This should leave about 150 images for analysis\n",
    "full_df.dropna(subset=[\"chl_a\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32e3e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 237 entries, 0 to 1709\n",
      "Data columns (total 33 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Grassland/Herbaceous       237 non-null    float64\n",
      " 1   Cultivated_Crops           237 non-null    float64\n",
      " 2   Shrub/Scrub                237 non-null    float64\n",
      " 3   BarrenLand_Rock/Sand/Clay  237 non-null    float64\n",
      " 4   Evergreen Forest           237 non-null    float64\n",
      " 5   Developed_OpenSpace        237 non-null    float64\n",
      " 6   Open_water                 237 non-null    float64\n",
      " 7   Class_sum                  237 non-null    float64\n",
      " 8   Woody_Wetlands             237 non-null    float64\n",
      " 9   qa                         237 non-null    int64  \n",
      " 10  chl_a                      237 non-null    float64\n",
      " 11  pwater                     237 non-null    float64\n",
      " 12  year                       237 non-null    int64  \n",
      " 13  dWL                        237 non-null    float64\n",
      " 14  Incid_Type                 237 non-null    object \n",
      " 15  BurnBndAc                  237 non-null    int64  \n",
      " 16  BurnBndLat                 237 non-null    float64\n",
      " 17  BurnBndLon                 237 non-null    float64\n",
      " 18  dNBR_offst                 237 non-null    int64  \n",
      " 19  aspect                     237 non-null    float64\n",
      " 20  chili                      237 non-null    int64  \n",
      " 21  elevation                  237 non-null    int64  \n",
      " 22  landform                   237 non-null    int64  \n",
      " 23  physd                      237 non-null    float64\n",
      " 24  slope                      237 non-null    float64\n",
      " 25  tdiv                       237 non-null    float64\n",
      " 26  wholearea                  237 non-null    float64\n",
      " 27  burnedarea                 237 non-null    float64\n",
      " 28  percentburned              237 non-null    float64\n",
      " 29  pre_post                   237 non-null    object \n",
      " 30  days_since                 237 non-null    int64  \n",
      " 31  img_month                  237 non-null    int64  \n",
      " 32  years_since                237 non-null    int64  \n",
      "dtypes: float64(21), int64(10), object(2)\n",
      "memory usage: 63.0+ KB\n"
     ]
    }
   ],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa90e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export file to local drive\n",
    "out_path = os.path.join(\"randomforest_df.csv\")\n",
    "full_df.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b570ecb1",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "Transformation of classification data to binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fb59b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 4, 511, 512]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 237 entries, 0 to 1709\n",
      "Data columns (total 37 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Grassland/Herbaceous          237 non-null    float64\n",
      " 1   Cultivated_Crops              237 non-null    float64\n",
      " 2   Shrub/Scrub                   237 non-null    float64\n",
      " 3   BarrenLand_Rock/Sand/Clay     237 non-null    float64\n",
      " 4   Evergreen Forest              237 non-null    float64\n",
      " 5   Developed_OpenSpace           237 non-null    float64\n",
      " 6   Open_water                    237 non-null    float64\n",
      " 7   Class_sum                     237 non-null    float64\n",
      " 8   Woody_Wetlands                237 non-null    float64\n",
      " 9   qa                            237 non-null    int64  \n",
      " 10  chl_a                         237 non-null    float64\n",
      " 11  pwater                        237 non-null    float64\n",
      " 12  year                          237 non-null    int64  \n",
      " 13  dWL                           237 non-null    float64\n",
      " 14  BurnBndAc                     237 non-null    int64  \n",
      " 15  BurnBndLat                    237 non-null    float64\n",
      " 16  BurnBndLon                    237 non-null    float64\n",
      " 17  dNBR_offst                    237 non-null    int64  \n",
      " 18  aspect                        237 non-null    float64\n",
      " 19  chili                         237 non-null    int64  \n",
      " 20  elevation                     237 non-null    int64  \n",
      " 21  landform                      237 non-null    int64  \n",
      " 22  physd                         237 non-null    float64\n",
      " 23  slope                         237 non-null    float64\n",
      " 24  tdiv                          237 non-null    float64\n",
      " 25  wholearea                     237 non-null    float64\n",
      " 26  burnedarea                    237 non-null    float64\n",
      " 27  percentburned                 237 non-null    float64\n",
      " 28  days_since                    237 non-null    int64  \n",
      " 29  img_month                     237 non-null    int64  \n",
      " 30  years_since                   237 non-null    int64  \n",
      " 31  Incid_Type_Prescribed Fire    237 non-null    uint8  \n",
      " 32  Incid_Type_Unknown            237 non-null    uint8  \n",
      " 33  Incid_Type_Wildfire           237 non-null    uint8  \n",
      " 34  Incid_Type_Wildland Fire Use  237 non-null    uint8  \n",
      " 35  pre_post_post-fire            237 non-null    uint8  \n",
      " 36  pre_post_pre-fire             237 non-null    uint8  \n",
      "dtypes: float64(21), int64(10), uint8(6)\n",
      "memory usage: 60.6 KB\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the data using pandas get_dummies\n",
    "formatted_df = pd.get_dummies(full_df)\n",
    "\n",
    "# Display some object columns to check transformation (fire type and pre or post fire)\n",
    "print(formatted_df.iloc[:,50:].head(5))\n",
    "\n",
    "formatted_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c08273",
   "metadata": {},
   "source": [
    "## Features and Targets and Convert Data to Arrays\n",
    "Separate the data into the features and targets. The target, \n",
    "also known as the label, is the value we want to predict, in this case the DWL (color) and the features are all the columns the model uses to make a prediction.\n",
    "We will also convert the Pandas dataframes to Numpy arrays because that is the way the algorithm works. \n",
    "(Save the column headers, which are the names of the features, to a list to use for later visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4099ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy to convert to arrays\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(formatted_df['dWL'])\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features = formatted_df.drop('dWL', axis = 1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b9c5096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ec672",
   "metadata": {},
   "source": [
    "### Training and Testing Sets\n",
    "There is one final step of data preparation: splitting data into training and testing sets. \n",
    "During training, we let the model ‘see’ the answers, in this case the actual dWL, so it can learn \n",
    "how to predict the temperature from the features. We expect there to be some relationship between \n",
    "all the features and the target value, and the model’s job is to learn this relationship during training. \n",
    "Then, when it comes time to evaluate the model, we ask it to make predictions on a testing set where \n",
    "it only has access to the features (not the answers)! Because we do have the actual answers for \n",
    "the test set, we can compare these predictions to the true value to judge how accurate the model is. \n",
    "Generally, when training a model, we randomly split the data into training and testing sets to get a \n",
    "representation of all data points (if we trained on the first nine months of the year and then used the \n",
    "final three months for prediction, our algorithm would not perform well because it has not seen any data \n",
    "from those last three months.) I am setting the random state to 42 which means the results will be the same \n",
    "each time I run the split for reproducible results.\n",
    "The following code splits the data sets with another single line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d48b241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c62fb19",
   "metadata": {},
   "source": [
    "We can look at the shape of all the data to make sure we did everything correctly. We expect the training features number of columns to match the testing feature number of columns and the number of rows to match for the respective training and testing features and the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad065eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (177, 36)\n",
      "Training Labels Shape: (177,)\n",
      "Testing Features Shape: (60, 36)\n",
      "Testing Labels Shape: (60,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67beb9b",
   "metadata": {},
   "source": [
    "Depending on the initial data set, there may be extra work involved such as removing outliers, imputing missing values, or converting temporal variables into cyclical representations. These steps may seem arbitrary at first, but once you get the basic workflow, it will be generally the same for any machine learning problem. It’s all about taking human-readable data and putting it into a form that can be understood by a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989cb4e4",
   "metadata": {},
   "source": [
    "# Establish Baseline\n",
    "Before we can make and evaluate predictions, we need to establish a baseline, a sensible measure that we hope to beat with our model. If our model cannot improve upon the baseline, then it will be a failure and we should try a different model or admit that machine learning is not right for our problem. The baseline prediction for our case can be the historical max temperature averages. In other words, our baseline is the error we would get if we simply predicted the average color for all images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f28d630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline predictions are the historical averages\n",
    "#baseline_preds = test_features[:, feature_list.index('Green')]\n",
    "\n",
    "# Baseline errors, and display average baseline error\n",
    "#baseline_errors = abs(baseline_preds - test_labels)\n",
    "#print('Average baseline error: ', round(np.mean(baseline_errors), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beae07c",
   "metadata": {},
   "source": [
    "We now have our goal! If we can’t beat an average error of 5 degrees, then we need to rethink our approach.\n",
    "\n",
    "## Train Model\n",
    "After all the work of data preparation, creating and training the model is pretty simple using Scikit-learn. We import the random forest regression model from skicit-learn, instantiate the model, and fit (scikit-learn’s name for training) the model on the training data. (Again setting the random state for reproducible results). This entire process is only 3 lines in scikit-learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5ed3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753472e5",
   "metadata": {},
   "source": [
    "## Make Predictions on the Test Set\n",
    "Our model has now been trained to learn the relationships between the features and the targets. The next step is figuring out how good the model is! To do this we make predictions on the test features (the model is never allowed to see the test answers). We then compare the predictions to the known answers. When performing regression, we need to make sure to use the absolute error because we expect some of our answers to be low and some to be high. We are interested in how far away our average prediction is from the actual value so we take the absolute value (as we also did when establishing the baseline).\n",
    "Making predictions with out model is another 1-line command in Skicit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34fffdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 8.92 nm.\n"
     ]
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'nm.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90320847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.68560606e-02 3.11815945e-01 1.54637097e-01 5.66308316e+00\n",
      " 5.04472272e-02 1.48221344e-02 4.26515303e-01 1.32382892e-01\n",
      " 7.28977273e-01 1.22403259e-01 6.68326613e+00 4.30970149e-02\n",
      " 7.92545432e+00 2.85923695e+00 7.70643939e-01 1.26353791e-03\n",
      " 7.24955752e-01 6.49857904e+00 0.00000000e+00 1.89203540e-01\n",
      " 1.45564516e-01 1.68375637e+00 1.32462687e-02 3.73216495e+00\n",
      " 2.13014761e-01 1.09164969e-01 9.88142292e-04 5.23262787e+00\n",
      " 4.14285714e-02 8.35183312e-01 1.17003891e+00 5.73709677e+00\n",
      " 1.16849315e+00 3.26548673e-01 5.95437650e+00 1.26353791e-03\n",
      " 2.74685817e-02 6.14031972e+00 2.14015152e-02 3.28571429e-02\n",
      " 2.75000000e-02 0.00000000e+00 1.12997248e-01 4.21428571e-02\n",
      " 3.03044355e+00 7.31225296e-03 1.53952641e+00 2.34850843e+00\n",
      " 6.66975806e+00 2.88223801e+00 3.19013646e-01 0.00000000e+00\n",
      " 3.27630522e+00 3.67857143e-02 0.00000000e+00 1.01977459e+01\n",
      " 1.56193896e-02 5.73165323e+00 0.00000000e+00 1.61579892e-02]\n",
      "Accuracy: 98.3 %.\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)\n",
    "print(mape)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c0ea053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "# if the desired path exists:\n",
    "data_dir = os.path.join(et.io.HOME, 'Dropbox',\n",
    "                        'cu_earthdata_certificate_2021', 'earthlab_project', 'ea-2021-final-project-wildfire_lake_productivity', 'images')\n",
    "\n",
    "os.chdir(data_dir)\n",
    "    \n",
    "# Export the image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "\n",
    "# Write graph to a png file\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e1f5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit depth of tree to 3 levels\n",
    "rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)\n",
    "rf_small.fit(train_features, train_labels)\n",
    "# Extract the small tree\n",
    "tree_small = rf_small.estimators_[5]\n",
    "# Save the tree as a png image\n",
    "export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\n",
    "graph.write_png('small_tree.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34f0e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: chl_a                Importance: 0.45\n",
      "Variable: year                 Importance: 0.13\n",
      "Variable: BurnBndLat           Importance: 0.07\n",
      "Variable: elevation            Importance: 0.06\n",
      "Variable: qa                   Importance: 0.03\n",
      "Variable: BurnBndAc            Importance: 0.03\n",
      "Variable: wholearea            Importance: 0.03\n",
      "Variable: days_since           Importance: 0.03\n",
      "Variable: BurnBndLon           Importance: 0.02\n",
      "Variable: dNBR_offst           Importance: 0.02\n",
      "Variable: burnedarea           Importance: 0.02\n",
      "Variable: percentburned        Importance: 0.02\n",
      "Variable: img_month            Importance: 0.02\n",
      "Variable: pwater               Importance: 0.01\n",
      "Variable: chili                Importance: 0.01\n",
      "Variable: landform             Importance: 0.01\n",
      "Variable: physd                Importance: 0.01\n",
      "Variable: slope                Importance: 0.01\n",
      "Variable: tdiv                 Importance: 0.01\n",
      "Variable: years_since          Importance: 0.01\n",
      "Variable: Grassland/Herbaceous Importance: 0.0\n",
      "Variable: Cultivated_Crops     Importance: 0.0\n",
      "Variable: Shrub/Scrub          Importance: 0.0\n",
      "Variable: BarrenLand_Rock/Sand/Clay Importance: 0.0\n",
      "Variable: Evergreen Forest     Importance: 0.0\n",
      "Variable: Developed_OpenSpace  Importance: 0.0\n",
      "Variable: Open_water           Importance: 0.0\n",
      "Variable: Class_sum            Importance: 0.0\n",
      "Variable: Woody_Wetlands       Importance: 0.0\n",
      "Variable: aspect               Importance: 0.0\n",
      "Variable: Incid_Type_Prescribed Fire Importance: 0.0\n",
      "Variable: Incid_Type_Unknown   Importance: 0.0\n",
      "Variable: Incid_Type_Wildfire  Importance: 0.0\n",
      "Variable: Incid_Type_Wildland Fire Use Importance: 0.0\n",
      "Variable: pre_post_post-fire   Importance: 0.0\n",
      "Variable: pre_post_pre-fire    Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
